# -*- coding: utf-8 -*-
"""Final_Notebook_Balfaqih_Kalliny_Qaisar

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tNJpATZHPOv7TWatxDZSsh7P0MY_Uq2E
"""

###################################################################
# University of Toronto
# Faculty of Information
# Master of Information Program
# INF 1340H - Programming for Data Science
#
# Student Name: Group 24 - Ahmed Balfaqih, David Kalliny, Abdullah Qaisar
# Student Number: 1012820930, 1008558614, 1003065416
# Supervisor: Dr. Maher Elshakankiri
#
#
# Final Project
# Purpose: This project provides a comprehensive data analysis pipeline for TTC (Toronto Transit Commission) Subway Delay Data from 2024 and 2025. The analysis follows a structured approach covering data preparation, descriptive analytics, diagnostic analytics, and predictive analytics to identify patterns, relationships, and predict delay severity.

# Date Created: 2025-10-22
# Date Modified: 2025-11-28
###################################################################

# ============================================================================
# IMPORTS
# ============================================================================

from typing import Optional, Dict, Tuple, Any
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

# ============================================================================
# CONFIGURATION
# ============================================================================

# Configure pandas display options for better readability
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.max_colwidth', 50)

# Configure matplotlib for better looking visualizations
plt.style.use('seaborn-v0_8-whitegrid' if 'seaborn-v0_8-whitegrid' in plt.style.available else 'seaborn-whitegrid')
plt.rcParams.update({
    'figure.facecolor': 'white',
    'axes.facecolor': 'white',
    'axes.edgecolor': '#333333',
    'axes.linewidth': 1.5,
    'axes.labelsize': 12,
    'axes.titlesize': 14,
    'axes.titleweight': 'bold',
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10,
    'legend.frameon': True,
    'legend.framealpha': 0.9,
    'legend.fancybox': True,
    'legend.shadow': True,
    'font.family': 'sans-serif',
    'font.sans-serif': ['Arial', 'DejaVu Sans', 'Liberation Sans'],
    'text.color': '#333333',
    'axes.labelcolor': '#333333',
    'xtick.color': '#333333',
    'ytick.color': '#333333',
    'grid.color': '#E0E0E0',
    'grid.alpha': 0.5,
    'grid.linewidth': 0.8,
    'figure.dpi': 100,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight'
})

# color palette
EXEC_COLORS = {
    'primary': '#2E86AB',      # blue
    'secondary': '#A23B72',    # purple
    'accent': '#F18F01',       # orange
    'success': '#06A77D',      # green
    'warning': '#D00000',      # red
    'neutral': '#6C757D',      # gray
    'light_blue': '#B8D4E3',
    'light_purple': '#D4B8D4',
    'light_green': '#B8E3D4'
}

# ============================================================================
# STEP 1: DATA LOADING FUNCTIONS
# ============================================================================

def load_ttc_data(file_path: str) -> Optional[pd.DataFrame]:
    """
    Load TTC subway delay data from CSV files with error handling.

    Args:
        file_path: Path to the CSV file to load

    Returns:
        Loaded DataFrame if successful, None if error occurs

    Raises:
        FileNotFoundError: If the file path does not exist
        Exception: For other file reading errors
    """
    try:
        df = pd.read_csv(file_path)
        print(f"Successfully loaded data from {file_path}")
        print(f"Shape: {df.shape[0]} rows, {df.shape[1]} columns")
        return df

    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return None
    except Exception as e:
        print(f"Error loading file: {str(e)}")
        return None


def inspect_dataframe(df: pd.DataFrame, df_name: str = "DataFrame") -> pd.DataFrame:
    """
    Inspect dataframe and display basic structural information about data quality.

    Displays:
    - Basic information (shape, dimensions)
    - Data types for each column
    - Column names
    - First 5 rows
    - Missing values summary
    - Empty strings count
    - Duplicate rows count

    Note: Statistical summaries are performed in the Descriptive Analytics section.

    Args:
        df: DataFrame to inspect
        df_name: Name of the dataframe for display purposes

    Returns:
        DataFrame containing missing values summary (columns with missing data only)
    """
    # Basic information
    print("\n1. Basic Information:")
    print(f"   Shape: {df.shape[0]} rows × {df.shape[1]} columns")

    # Data types
    print("\n2. Data Types:")
    print(df.dtypes)

    # Column names
    print("\n3. Column Names:")
    print(list(df.columns))

    # First few rows
    print("\n4. First 5 Rows:")
    print(df.head())

    # Missing values
    print("\n5. Missing Values:")
    missing_data = df.isnull().sum()
    missing_percent = (missing_data / len(df)) * 100
    missing_df = pd.DataFrame({
        'Missing Count': missing_data,
        'Percentage': missing_percent
    })
    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)
    if len(missing_df) > 0:
        print(missing_df)
    else:
        print("   No missing values found!")

    # Check for empty strings
    print("\n6. Empty Strings:")
    empty_strings: Dict[str, int] = {}
    for col in df.columns:
        if df[col].dtype == 'object':
            empty_count = (df[col] == '').sum()
            if empty_count > 0:
                empty_strings[col] = empty_count
    if empty_strings:
        for col, count in empty_strings.items():
            print(f"   {col}: {count} empty strings ({(count/len(df)*100):.2f}%)")
    else:
        print("   No empty strings found!")

    # Duplicate rows
    print("\n7. Duplicate Rows:")
    duplicate_count = df.duplicated().sum()
    print(f"   Total duplicates: {duplicate_count} ({(duplicate_count/len(df)*100):.2f}%)")

    return missing_df

# ============================================================================
# STEP 2: DATA CLEANING FUNCTIONS
# ============================================================================

def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    Standardize column names to lowercase with underscores.

    Converts all column names to lowercase and replaces spaces with underscores.
    Example: "Min Delay" --> "min_delay"

    Args:
        df: Input DataFrame with original column names

    Returns:
        DataFrame with standardized column names

    Example:
        >> df = pd.DataFrame({'Min Delay': [1, 2], 'Min Gap': [3, 4]})
        >> df_cleaned = standardize_column_names(df)
        >> print(df_cleaned.columns.tolist())
        ['min_delay', 'min_gap']
    """
    print("  -> 1.4.0: Standardizing column names (Resolve data inconsistencies)")
    df_cleaned = df.copy()
    original_cols = list(df_cleaned.columns)
    df_cleaned.columns = df_cleaned.columns.str.lower().str.replace(' ', '_')
    print(f"    Standardized {len(original_cols)} column names")
    return df_cleaned


def convert_data_types(df: pd.DataFrame) -> pd.DataFrame:
    """
    Convert columns to appropriate data types for analysis.

    Converts:
    - Date column to datetime
    - Numeric columns (min_delay, min_gap, vehicle) to numeric
    - Categorical columns (day, station, code, bound, line) to category type

    Args:
        df: Input DataFrame

    Returns:
        DataFrame with converted data types
    """
    print("  -> 1.5.0: Converting data types (Transform and normalize variables)")
    df_cleaned = df.copy()
    converted = []

    # Convert Date column to datetime
    if 'date' in df_cleaned.columns:
        df_cleaned['date'] = pd.to_datetime(df_cleaned['date'], errors='coerce')
        converted.append('date (datetime)')

    # Convert numeric columns
    numeric_columns = ['min_delay', 'min_gap', 'vehicle']
    numeric_converted = []
    for col in numeric_columns:
        if col in df_cleaned.columns:
            df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')
            numeric_converted.append(col)
    if numeric_converted:
        converted.append(f"{', '.join(numeric_converted)} (numeric)")

    # Convert categorical columns
    categorical_columns = ['day', 'station', 'code', 'bound', 'line']
    cat_converted = []
    for col in categorical_columns:
        if col in df_cleaned.columns:
            df_cleaned[col] = df_cleaned[col].astype('category')
            cat_converted.append(col)
    if cat_converted:
        converted.append(f"{', '.join(cat_converted)} (category)")

    print(f"    Converted data types: {', '.join(converted)}")
    return df_cleaned


def handle_missing_values(
    df: pd.DataFrame,
    strategy: str = 'remove',
    create_flags: bool = True
) -> pd.DataFrame:
    """
    Handle missing values in the dataframe using specified strategy.

    Strategies:
    - 'remove': Remove rows with missing critical values (date, station, line, min_delay)
    - 'impute': Impute missing values using median (numeric) or mode (categorical)
    - 'flag': Create flag columns for missing values

    Args:
        df: Input DataFrame
        strategy: Strategy to use ('remove', 'impute', or 'flag')
        create_flags: Whether to create missing flags for useful columns (bound, line)

    Returns:
        DataFrame with missing values handled according to strategy
    """
    print("  -> 1.2.0: Handling missing values (strategy: remove)")
    df_cleaned = df.copy()
    initial_count = len(df_cleaned)

    if strategy == 'remove':
        # Remove rows with missing critical information
        critical_columns = ['date', 'station', 'line']
        if all(col in df_cleaned.columns for col in critical_columns):
            before_critical = len(df_cleaned)
            df_cleaned = df_cleaned.dropna(subset=critical_columns, how='all')
            removed_critical = before_critical - len(df_cleaned)
            if removed_critical > 0:
                print(f"    Removed {removed_critical:,} rows with all critical columns missing")

        # Remove rows where Min Delay is missing
        if 'min_delay' in df_cleaned.columns:
            before_delay = len(df_cleaned)
            df_cleaned = df_cleaned.dropna(subset=['min_delay'])
            removed_delay = before_delay - len(df_cleaned)
            if removed_delay > 0:
                print(f"    Removed {removed_delay:,} rows with missing min_delay")

        # Create missing flags
        if create_flags:
            useful_flag_columns = ['bound', 'line']
            flags_created = 0
            for col in useful_flag_columns:
                if col in df_cleaned.columns and df_cleaned[col].isnull().sum() > 0:
                    df_cleaned[f'{col}_missing'] = df_cleaned[col].isnull().astype(int)
                    flags_created += 1
            if flags_created > 0:
                print(f"    Created {flags_created} missing value flag column(s)")

        final_count = len(df_cleaned)
        removed = initial_count - final_count
        if removed > 0:
            print(f"    Total rows removed: {removed:,}")

    elif strategy == 'impute':
        # Impute missing values based on column type
        numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if df_cleaned[col].isnull().sum() > 0:
                df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)

        categorical_columns = df_cleaned.select_dtypes(include=['category', 'object']).columns
        for col in categorical_columns:
            if df_cleaned[col].isnull().sum() > 0:
                mode_value = df_cleaned[col].mode()[0] if len(df_cleaned[col].mode()) > 0 else 'Unknown'
                df_cleaned[col].fillna(mode_value, inplace=True)

    elif strategy == 'flag':
        # Create flags for missing values
        for col in df_cleaned.columns:
            if df_cleaned[col].isnull().sum() > 0:
                df_cleaned[f'{col}_missing'] = df_cleaned[col].isnull().astype(int)

    return df_cleaned


def handle_empty_strings(df: pd.DataFrame) -> pd.DataFrame:
    """
    Replace empty strings and 'None' strings with NaN in object columns.

    Args:
        df: Input DataFrame

    Returns:
        DataFrame with empty strings replaced by NaN
    """
    print("  -> 1.4.0: Handling empty strings (Resolve data inconsistencies)")
    df_cleaned = df.copy()
    total_empty = 0

    # Replace empty strings in object columns with NaN
    for col in df_cleaned.select_dtypes(include=['object']).columns:
        empty_count = (df_cleaned[col] == '').sum()

        if empty_count > 0:
            df_cleaned[col] = df_cleaned[col].replace('', np.nan)
            # Handle 'None' as string
            df_cleaned[col] = df_cleaned[col].replace('None', np.nan)
            total_empty += empty_count

    if total_empty > 0:
        print(f"    Replaced {total_empty:,} empty strings with NaN across object columns")
    else:
        print("    No empty strings found")

    return df_cleaned


def filter_subway_records(df: pd.DataFrame) -> pd.DataFrame:
    """
    Filter to keep only subway records (exclude bus, streetcar, etc.).

    Filters to subway lines: YU (Yonge-University), BD (Bloor-Danforth), SHP (Sheppard)

    Args:
        df: Input DataFrame

    Returns:
        DataFrame containing only subway line records
    """
    print("  -> 1.4.0: Filtering to subway records only (Resolve data inconsistencies)")
    df_cleaned = df.copy()
    initial_count = len(df_cleaned)

    if 'line' in df_cleaned.columns:
        # Standardize line names: YU (Yonge-University), BD (Bloor-Danforth), SHP (Sheppard)
        df_cleaned['line'] = df_cleaned['line'].astype(str).str.upper().str.strip()

        # Filter to keep only known subway lines (YU, BD, SHP)
        subway_mask = df_cleaned['line'].isin(['YU', 'BD', 'SHP'])
        df_cleaned = df_cleaned[subway_mask]

        final_count = len(df_cleaned)
        removed = initial_count - final_count
        if removed > 0:
            print(f"    Removed {removed:,} non-subway records ({removed/initial_count*100:.2f}%)")
            print(f"    Kept {final_count:,} subway records (YU, BD, SHP)")
        else:
            print(f"    All {final_count:,} records are subway records")
    else:
        print("    Warning: 'line' column not found, skipping filter")

    return df_cleaned


def standardize_categorical_values(df: pd.DataFrame) -> pd.DataFrame:
    """
    Standardize categorical values and handle station name variants.

    Standardizes:
    - Day names: Capitalizes consistently
    - Station names: Uppercase, removes extra spaces, fixes common variants
    - Bound values: Converts to uppercase, replaces "NONE"/"NAN" with NaN
    - Line labels: Standardizes variations (e.g., "YONGE-UNIVERSITY" --> "YU")

    Args:
        df: Input DataFrame

    Returns:
        DataFrame with standardized categorical values
    """
    print("  -> 1.4.0: Standardizing categorical values (Resolve data inconsistencies)")
    df_cleaned = df.copy()
    standardized = []

    # Standardize Day names
    if 'day' in df_cleaned.columns:
        df_cleaned['day'] = df_cleaned['day'].astype(str).str.capitalize()
        standardized.append('day')

    # Standardize Station names (uppercase, remove extra spaces, fix common variants)
    if 'station' in df_cleaned.columns:
        df_cleaned['station'] = df_cleaned['station'].astype(str).str.upper().str.strip()
        # Remove extra spaces
        df_cleaned['station'] = df_cleaned['station'].str.replace(r'\s+', ' ', regex=True)
        # Fix common abbreviations and variants
        station_fixes = {
            'KENNEDY BD': 'KENNEDY BD STATION',
            'KENNEDY': 'KENNEDY BD STATION',
            'ST.': 'ST',
            'STREET': 'ST',
        }
        for old, new in station_fixes.items():
            df_cleaned['station'] = df_cleaned['station'].str.replace(old, new, regex=False)
        standardized.append('station')

    # Standardize Bound values
    if 'bound' in df_cleaned.columns:
        df_cleaned['bound'] = df_cleaned['bound'].astype(str).str.upper().str.strip()
        df_cleaned['bound'] = df_cleaned['bound'].replace(['NONE', 'NAN', ''], np.nan)
        standardized.append('bound')

    # Standardize Line values
    if 'line' in df_cleaned.columns:
        df_cleaned['line'] = df_cleaned['line'].astype(str).str.upper().str.strip()
        # Fix line label inconsistencies
        line_fixes = {
            'YONGE-UNIVERSITY': 'YU',
            'BLOOR-DANFORTH': 'BD',
            'SHEPPARD': 'SHP',
            'YONGE': 'YU',
            'BLOOR': 'BD',
            'DANFORTH': 'BD',
        }
        for old, new in line_fixes.items():
            df_cleaned['line'] = df_cleaned['line'].str.replace(old, new, regex=False)
        standardized.append('line')

    print(f"    Standardized {len(standardized)} categorical columns: {', '.join(standardized)}")
    return df_cleaned


def handle_data_errors(
    df: pd.DataFrame,
    column: str,
    use_percentile: bool = True,
    percentile: int = 99
) -> pd.DataFrame:
    """
    Remove obvious data errors (negative values, extreme values) using percentile cap.

    For min_delay and min_gap columns:
    - Removes negative values
    - Caps extreme values at specified percentile (default 99th)

    Args:
        df: Input DataFrame
        column: Column name to check for errors
        use_percentile: Whether to use percentile capping
        percentile: Percentile to use for capping (default: 99)

    Returns:
        DataFrame with data errors handled
    """
    df_cleaned = df.copy()
    initial_count = len(df_cleaned)

    if column not in df_cleaned.columns:
        return df_cleaned

    # Remove negative values
    if column in ['min_delay', 'min_gap']:
        negative_count = (df_cleaned[column] < 0).sum()
        if negative_count > 0:
            df_cleaned = df_cleaned[df_cleaned[column] >= 0]
            print(f"    Removed {negative_count:,} rows with negative {column} values")

    # Cap extreme values using percentile
    if use_percentile:
        upper_bound = df_cleaned[column].quantile(percentile / 100)
        lower_bound = 0 if column in ['min_delay', 'min_gap'] else df_cleaned[column].quantile((100 - percentile) / 100)

        # Count values that will be capped
        extreme_count = ((df_cleaned[column] > upper_bound) | (df_cleaned[column] < lower_bound)).sum()

        # Cap at bounds
        df_cleaned[column] = df_cleaned[column].clip(lower=lower_bound, upper=upper_bound)

        if extreme_count > 0:
            print(f"    Capped {extreme_count:,} extreme values in {column} at {percentile}th percentile (upper: {upper_bound:.2f})")

    final_count = len(df_cleaned)
    removed = initial_count - final_count
    if removed > 0:
        print(f"    Total rows removed from {column}: {removed:,}")

    return df_cleaned


def create_datetime_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    Create a combined datetime column from Date and Time columns.

    Also extracts additional time features:
    - hour: Hour of day (0-23)
    - month: Month (1-12)
    - year: Year
    - day_of_week: Numeric day (0=Monday, 6=Sunday)
    - weekday: Day name (Monday-Sunday)
    - is_peak: Peak period flag (1 if 7-10am OR 4-7pm, 0 otherwise)
    - is_morning_peak: Morning peak flag (1 if 7-10am, 0 otherwise)
    - is_evening_peak: Evening peak flag (1 if 4-7pm, 0 otherwise)

    Args:
        df: Input DataFrame with 'date' and 'time' columns

    Returns:
        DataFrame with datetime column and extracted time features
    """
    print("  -> 1.5.0: Creating datetime features (Transform and normalize variables)")
    df_cleaned = df.copy()

    if 'date' in df_cleaned.columns and 'time' in df_cleaned.columns:
        # Combine date and time
        df_cleaned['datetime'] = pd.to_datetime(
            df_cleaned['date'].astype(str) + ' ' + df_cleaned['time'].astype(str),
            errors='coerce'
        )

        invalid_datetime = df_cleaned['datetime'].isnull().sum()
        if invalid_datetime > 0:
            print(f"    Warning: {invalid_datetime:,} rows have invalid datetime values")

        # Extract additional time features
        df_cleaned['hour'] = df_cleaned['datetime'].dt.hour
        df_cleaned['month'] = df_cleaned['datetime'].dt.month
        df_cleaned['year'] = df_cleaned['datetime'].dt.year
        df_cleaned['day_of_week'] = df_cleaned['datetime'].dt.dayofweek

        # Create weekday name (0=Monday, 6=Sunday)
        weekday_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday',
                        4: 'Friday', 5: 'Saturday', 6: 'Sunday'}
        df_cleaned['weekday'] = df_cleaned['day_of_week'].map(weekday_names)

        # Create peak flag (7-10am morning peak, 4-7pm evening peak)
        df_cleaned['is_peak'] = ((df_cleaned['hour'] >= 7) & (df_cleaned['hour'] < 10)) | \
                                ((df_cleaned['hour'] >= 16) & (df_cleaned['hour'] < 19))
        df_cleaned['is_peak'] = df_cleaned['is_peak'].astype(int)

        # Create separate morning and evening peak flags
        df_cleaned['is_morning_peak'] = ((df_cleaned['hour'] >= 7) & (df_cleaned['hour'] < 10)).astype(int)
        df_cleaned['is_evening_peak'] = ((df_cleaned['hour'] >= 16) & (df_cleaned['hour'] < 19)).astype(int)

        peak_count = df_cleaned['is_peak'].sum()
        print(f"    Created datetime features: hour, month, year, day_of_week, weekday, is_peak, is_morning_peak, is_evening_peak")
        print(f"    Peak hours identified: {peak_count:,} rows ({peak_count/len(df_cleaned)*100:.1f}%)")
    else:
        print("    Warning: 'date' and/or 'time' columns not found")

    return df_cleaned


def clean_ttc_data(
    df: pd.DataFrame,
    remove_outliers: bool = False,
    outlier_method: str = 'cap',
    filter_subway: bool = True
) -> pd.DataFrame:
    """
    Main data cleaning function that applies all cleaning steps in sequence.

    Cleaning steps:
    1. Standardize column names (lowercase with underscores)
    2. Handle empty strings (replace with NaN)
    3. Convert data types (dates --> datetime, minutes --> numeric, categories --> category)
    4. Filter to subway records only (YU, BD, SHP)
    5. Standardize categorical values (fix inconsistencies)
    6. Handle missing values (remove critical missing, create flags)
    7. Create datetime column and extract time features
    8. Handle data errors (remove negatives, cap extremes at 99th percentile)
    9. Remove records with zero delay or zero gap (not relevant for delay analysis)

    Args:
        df: Input DataFrame with raw TTC delay data
        remove_outliers: Whether to remove outliers (not currently used)
        outlier_method: Method for handling outliers (not currently used)
        filter_subway: Whether to filter to subway records only

    Returns:
        Cleaned DataFrame ready for analysis
    """
    df_cleaned = df.copy()
    initial_rows = len(df_cleaned)

    # Cleaning Step 1: Standardize column names
    df_cleaned = standardize_column_names(df_cleaned)

    # Cleaning Step 2: Handle empty strings
    df_cleaned = handle_empty_strings(df_cleaned)

    # Cleaning Step 3: Convert data types
    df_cleaned = convert_data_types(df_cleaned)

    # Cleaning Step 4: Filter to subway records only
    if filter_subway:
        df_cleaned = filter_subway_records(df_cleaned)

    # Cleaning Step 5: Standardize categorical values
    df_cleaned = standardize_categorical_values(df_cleaned)

    # Cleaning Step 6: Handle missing values
    df_cleaned = handle_missing_values(df_cleaned, strategy='remove', create_flags=True)

    # Cleaning Step 7: Create datetime column and extract features
    df_cleaned = create_datetime_column(df_cleaned)

    # Cleaning Step 8: Remove obvious data errors
    print("  -> 1.3.0: Handling data errors - Detect and handle outliers (negatives and extremes)")
    if 'min_delay' in df_cleaned.columns:
        df_cleaned = handle_data_errors(df_cleaned, 'min_delay', use_percentile=True, percentile=99)

    if 'min_gap' in df_cleaned.columns:
        df_cleaned = handle_data_errors(df_cleaned, 'min_gap', use_percentile=True, percentile=99)

    # Cleaning Step 9: Remove records with zero delay or zero gap
    print("  -> 1.5.0: Removing zero delays and gaps (Transform and normalize variables)")
    before_zero = len(df_cleaned)
    # Remove records with no delay or no gap
    if 'min_delay' in df_cleaned.columns:
        df_cleaned = df_cleaned[df_cleaned['min_delay'] > 0].copy()
        removed_delay = before_zero - len(df_cleaned)
        if removed_delay > 0:
            print(f"    Removed {removed_delay:,} rows with zero min_delay")

    if 'min_gap' in df_cleaned.columns:
        before_gap = len(df_cleaned)
        df_cleaned = df_cleaned[df_cleaned['min_gap'] > 0].copy()
        removed_gap = before_gap - len(df_cleaned)
        if removed_gap > 0:
            print(f"    Removed {removed_gap:,} rows with zero min_gap")

    # Cleaning Final summary
    final_rows = len(df_cleaned)
    removed_rows = initial_rows - final_rows

    print("\n" + "=" * 70)
    print("Data Cleaning Complete!")
    print("=" * 70)
    print(f"Initial rows: {initial_rows:,}")
    print(f"Final rows: {final_rows:,}")
    print(f"Rows removed: {removed_rows:,} ({(removed_rows/initial_rows*100):.2f}%)")
    print(f"Data retention: {((final_rows/initial_rows)*100):.2f}%")

    return df_cleaned

# ============================================================================
# STEP 3: DESCRIPTIVE ANALYTICS FUNCTIONS
# ============================================================================

def calculate_summary_statistics(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate comprehensive summary statistics for numerical columns.

    Calculates: mean, median, mode, standard deviation, min, max, quartiles,
    skewness, and kurtosis for all numerical columns.

    Args:
        df: Input DataFrame

    Returns:
        DataFrame with summary statistics for each numerical column
    """
    numeric_cols = df.select_dtypes(include=[np.number]).columns

    if len(numeric_cols) == 0:
        return pd.DataFrame()

    summary_stats = pd.DataFrame({
        'mean': df[numeric_cols].mean(),
        'median': df[numeric_cols].median(),
        'std': df[numeric_cols].std(),
        'min': df[numeric_cols].min(),
        'max': df[numeric_cols].max(),
        'q25': df[numeric_cols].quantile(0.25),
        'q75': df[numeric_cols].quantile(0.75),
        'skewness': df[numeric_cols].skew(),
        'kurtosis': df[numeric_cols].kurtosis(),
    })

    # Calculate mode for each column
    modes = []
    for col in numeric_cols:
        mode_values = df[col].mode()
        if len(mode_values) > 0:
            modes.append(mode_values[0])
        else:
            modes.append(np.nan)
    summary_stats['mode'] = modes

    return summary_stats


def calculate_missing_value_proportions(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate the proportion of missing values for each variable.

    Args:
        df: Input DataFrame

    Returns:
        DataFrame with missing value counts and proportions for each column
    """
    missing_data = df.isnull().sum()
    missing_percent = (missing_data / len(df)) * 100

    missing_df = pd.DataFrame({
        'Missing_Count': missing_data,
        'Missing_Proportion': missing_percent / 100,
        'Missing_Percentage': missing_percent
    })

    # Sort by missing count descending
    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values(
        'Missing_Count', ascending=False
    )

    return missing_df


def generate_frequency_distributions(df: pd.DataFrame, categorical_cols: Optional[list] = None) -> Dict[str, pd.Series]:
    """
    Generate frequency distributions for categorical data.

    Args:
        df: Input DataFrame
        categorical_cols: List of categorical column names. If None, auto-detect.

    Returns:
        Dictionary mapping column names to their frequency distributions
    """
    if categorical_cols is None:
        categorical_cols = df.select_dtypes(include=['category', 'object']).columns.tolist()

    frequency_distributions = {}

    for col in categorical_cols:
        if col in df.columns:
            freq_dist = df[col].value_counts().sort_values(ascending=False)
            frequency_distributions[col] = freq_dist

    return frequency_distributions


def segment_data_by_category(
    df: pd.DataFrame,
    segment_col: str,
    numeric_cols: Optional[list] = None
) -> pd.DataFrame:
    """
    Segment the data by relevant categories and calculate statistics for each segment.

    Args:
        df: Input DataFrame
        segment_col: Column name to segment by (e.g., 'line', 'station', 'weekday')
        numeric_cols: List of numeric columns to analyze. If None, auto-detect.

    Returns:
        DataFrame with summary statistics for each segment
    """
    if segment_col not in df.columns:
        return pd.DataFrame()

    if numeric_cols is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    # Group by segment column and calculate statistics
    segment_stats = df.groupby(segment_col)[numeric_cols].agg([
        'count', 'mean', 'median', 'std', 'min', 'max'
    ]).round(2)

    return segment_stats


def run_descriptive_analytics(df_cleaned: pd.DataFrame) -> Dict[str, Any]:
    """
    Run comprehensive descriptive analytics on cleaned TTC delay data.

    Performs:
    1. Summary statistics (mean, median, mode, std dev, skewness, kurtosis)
    2. Missing value proportions
    3. Frequency distributions for categorical data
    4. Data segmentation by relevant categories (line, station, weekday, etc.)

    Args:
        df_cleaned: Cleaned DataFrame from the data cleaning pipeline

    Returns:
        Dictionary containing:
        - summary_stats: Summary statistics DataFrame
        - missing_proportions: Missing value proportions DataFrame
        - frequency_distributions: Dictionary of frequency distributions
        - segment_stats: Dictionary of segment statistics by category
    """
    print("\n" + "=" * 70)
    print("2.0.0 DESCRIPTIVE ANALYTICS")
    print("=" * 70)

    results: Dict[str, Any] = {}

    # 2.1.0 Summary Statistics
    print("\n" + "-" * 60)
    print("2.1.0 Summary Statistics for Numerical Variables")
    print("(See Figure 2 for visual distribution analysis)")
    print("-" * 60)
    summary_stats = calculate_summary_statistics(df_cleaned)
    if not summary_stats.empty:
        print(summary_stats)
        results['summary_stats'] = summary_stats
    else:
        print("No numerical columns found.")
        results['summary_stats'] = pd.DataFrame()

    # 2.2.0 Missing Value Proportions
    print("\n" + "-" * 60)
    print("2.2.0 Missing Value Proportions")
    print("-" * 60)
    missing_proportions = calculate_missing_value_proportions(df_cleaned)
    if not missing_proportions.empty:
        print(missing_proportions)
        results['missing_proportions'] = missing_proportions
    else:
        print("No missing values found.")
        results['missing_proportions'] = pd.DataFrame()

    # 2.3.0 Frequency Distributions for Categorical Data
    print("\n" + "-" * 60)
    print("2.3.0 Frequency Distributions for Categorical Variables")
    print("-" * 60)
    categorical_cols = ['line', 'station', 'code', 'weekday', 'bound']
    frequency_distributions = generate_frequency_distributions(df_cleaned, categorical_cols)

    for col, freq_dist in frequency_distributions.items():
        print(f"\n{col.upper()} Frequency Distribution (Top 10):")
        print(freq_dist.head(10))

    results['frequency_distributions'] = frequency_distributions

    # 2.4.0 Data Segmentation by Categories
    print("\n" + "-" * 60)
    print("2.4.0 Data Segmentation by Categories")
    print("(See Figure 3 for comparative visualizations across categories)")
    print("-" * 60)

    segment_stats = {}

    # Segment by Line
    if 'line' in df_cleaned.columns:
        print("\n" + "-" * 50)
        print("2.4.1 Segmentation by Line:")
        print("-" * 50)
        line_stats = segment_data_by_category(df_cleaned, 'line', ['min_delay', 'min_gap'])
        if not line_stats.empty:
            print(line_stats)
            segment_stats['by_line'] = line_stats

    # Segment by Weekday
    if 'weekday' in df_cleaned.columns:
        print("\n" + "-" * 50)
        print("2.4.2 Segmentation by Weekday:")
        print("-" * 50)
        weekday_stats = segment_data_by_category(df_cleaned, 'weekday', ['min_delay', 'min_gap'])
        if not weekday_stats.empty:
            print(weekday_stats)
            segment_stats['by_weekday'] = weekday_stats

    # Segment by Peak vs Off-Peak
    if 'is_peak' in df_cleaned.columns:
        print("\n" + "-" * 50)
        print("2.4.3 Segmentation by Peak vs Off-Peak:")
        print("-" * 50)
        peak_stats = segment_data_by_category(df_cleaned, 'is_peak', ['min_delay', 'min_gap'])
        if not peak_stats.empty:
            print(peak_stats)
            segment_stats['by_peak'] = peak_stats

    # Segment by Month
    if 'month' in df_cleaned.columns:
        print("\n" + "-" * 50)
        print("2.4.4 Segmentation by Month:")
        print("-" * 50)
        month_stats = segment_data_by_category(df_cleaned, 'month', ['min_delay', 'min_gap'])
        if not month_stats.empty:
            print(month_stats)
            segment_stats['by_month'] = month_stats

    results['segment_stats'] = segment_stats

    # 2.5.0 Visualizations for Descriptive Analytics
    print("\n" + "-" * 60)
    print("2.5.0 Visualizations: Delay Distributions")
    print("Figure 2: Visual analysis of delay distributions across lines, peak times, and weekdays")
    print("-" * 60)
    visualize_delay_distributions(df_cleaned)

    print("\n" + "-" * 50)
    print("2.5.1 Visualizations: Comparative Analysis")
    print("Figure 3: Comparative analysis across lines, stations, and delay codes")
    print("-" * 50)
    visualize_comparative_analysis(df_cleaned)

    print("\n" + "-" * 50)
    print("2.5.2 Visualizations: Trends Over Time")
    print("Figure 4: Time series analysis showing monthly, hourly, and weekday patterns")
    print("-" * 50)
    visualize_trends_over_time(df_cleaned)

    print("\n" + "=" * 70)
    print("[Done] 2.0.0 Descriptive analytics complete.")
    print("=" * 70 + "\n")

    return results

# ============================================================================
# STEP 4: DIAGNOSTIC ANALYTICS FUNCTIONS
# ============================================================================

def display_code_categories() -> None:
    """
    Display a table explaining TTC delay code categories.

    Shows the prefix codes (MU, PU, EU, TU) and their meanings:
    - MU: Train mechanical issues
    - PU: Passenger-related delays
    - EU: Signal/electrical failures
    - TU: Tunnel/track issues
    """
    print("=====================================")
    print("        KEY: TTC CODE CATEGORIES")
    print("=====================================")

    code_table = pd.DataFrame({
        "Prefix": ["MU", "PU", "EU", "TU"],
        "Examples": [
            "MUI, MUIS, MUPAA",
            "PUTDN",
            "EUCD, EUBK, EUBO",
            "TUNOA, TUO, TUOPO, TUOS"
        ],
        "Meaning": [
            "Train mechanical issues",
            "Passenger-related delays",
            "Signal/electrical failures",
            "Tunnel/track issues"
        ]
    })

    print(code_table.to_string(index=False))


def run_diagnostic_analytics(df_cleaned: pd.DataFrame) -> Dict[str, Any]:
    """
    Perform diagnostic analytics on cleaned TTC delay data.

    Diagnostic analytics answers "WHY are these delays happening?" by looking for
    patterns and relationships in the data.

    Analyses performed:
    1. Correlation analysis for numeric variables
    2. Cross-tabulation analysis for categorical variables (Line x Code, Station x Peak)
    3. Linear regression: Impact of Train Headway (Min Gap) on Delay Duration (Min Delay)
    4. T-test: Comparison of peak vs off-peak mean delays

    Args:
        df_cleaned: Cleaned DataFrame from the data cleaning pipeline.
                   Must contain columns: min_delay, min_gap, line, station, code, is_peak

    Returns:
        Dictionary containing:
        - corr_numeric: Correlation matrix for numeric columns
        - xtab_line_cause: Cross-tabulation of Line x Code
        - xtab_station_peak: Cross-tabulation of Station x is_peak
        - regression_gap_delay: Regression results (slope, intercept, r-value, p-value, etc.)
        - ttest_peak_offpeak: T-test results (mean_peak, mean_offpeak, t_stat, pvalue)
    """
    print("\n" + "=" * 70)
    print("3.0.0 DIAGNOSTIC ANALYTICS")
    print("=" * 70)
    print("Diagnostic analytics means asking: WHY are these delays happening,")
    print("by looking for patterns and relationships in the data.")

    # Map column names (cleaned data uses lowercase with underscores)
    # The cleaned dataframe from clean_ttc_data() will have lowercase column names
    delay_col = 'min_delay' if 'min_delay' in df_cleaned.columns else 'Min Delay'
    gap_col = 'min_gap' if 'min_gap' in df_cleaned.columns else 'Min Gap'
    line_col = 'line' if 'line' in df_cleaned.columns else 'Line'
    station_col = 'station' if 'station' in df_cleaned.columns else 'Station'
    cause_col = 'code' if 'code' in df_cleaned.columns else 'Code'
    peak_col = 'is_peak' if 'is_peak' in df_cleaned.columns else 'is_peak'

    results: Dict[str, Any] = {}

    # 3.1.0 CORRELATION ANALYSIS (NUMERIC COLUMNS)
    print("\n" + "-" * 60)
    print("3.1.0 Correlation Analysis (Numeric)")
    print("(See Figure 5 for correlation heatmap visualization)")
    print("-" * 60)
    print("Which numbers move together and might help explain delays?")

    # Select only numeric columns
    num_cols = df_cleaned.select_dtypes(include="number").columns

    print("Numeric columns we will use:")
    print(list(num_cols))

    # Calculate correlation matrix
    corr_numeric = df_cleaned[num_cols].corr().round(3)

    print("\nCorrelation matrix:")
    print(corr_numeric)

    results["corr_numeric"] = corr_numeric

    # Visualize correlation matrix
    print("\nCorrelation Heatmap Visualization:")
    print("Figure 5: Correlation heatmap showing relationships between numeric variables")
    visualize_correlation_heatmap(df_cleaned)

    # 3.2.0 CROSS-TABS (CATEGORICAL COLUMNS)
    print("\n" + "-" * 60)
    print("3.2.0 Categorical Analysis: Line & Station Delay Analysis")
    print("-" * 60)
    print("Where delays are happening most and under what conditions?")

    # Line x Code cross-tab
    if (line_col in df_cleaned.columns) and (cause_col in df_cleaned.columns):
        xtab_line_cause = pd.crosstab(
            df_cleaned[line_col],
            df_cleaned[cause_col]
        )

        print("\n" + "-" * 50)
        print("3.2.1 Cross-tab: Line x Code (counts)")
        print("-" * 50)
        print("(Only showing first few rows):")
        print(xtab_line_cause.head())

        results["xtab_line_cause"] = xtab_line_cause
    else:
        print("\n[Warning] Line or Code name column not found.")
        results["xtab_line_cause"] = None

    # Station x is_peak cross-tab
    if (station_col in df_cleaned.columns) and (peak_col in df_cleaned.columns):
        xtab_station_peak = pd.crosstab(
            df_cleaned[station_col],
            df_cleaned[peak_col]
        )

        print("\n" + "-" * 50)
        print("3.2.2 Cross-tab: Station x is_peak (counts)")
        print("-" * 50)
        print("(Only showing first few rows):")
        print(xtab_station_peak.head())

        results["xtab_station_peak"] = xtab_station_peak
    else:
        print("\n[Warning] Station or is_peak column not found.")
        results["xtab_station_peak"] = None

    # 3.3.0 REGRESSION
    print("\n" + "-" * 60)
    print("3.3.0 Regression: Impact of Train Headway (Min Gap) on Delay Duration (Min Delay)")
    print("(See Figure 6 for regression scatter plot)")
    print("-" * 60)
    print("Does bigger Min Gap give bigger Min Delay?")

    if (gap_col in df_cleaned.columns) and (delay_col in df_cleaned.columns):
        # Keep only rows where both Min Gap and Min Delay are not missing
        reg_df = df_cleaned[[gap_col, delay_col]].dropna()

        x = reg_df[gap_col].values
        y = reg_df[delay_col].values

        if len(reg_df) > 2:
            reg = stats.linregress(x, y)

            print(f"\nRegression of {delay_col} on {gap_col}:")
            print("  slope      :", reg.slope)
            print("  intercept  :", reg.intercept)
            print("  r-value    :", reg.rvalue)
            print("  r^2        :", reg.rvalue ** 2)
            print("  p-value    :", reg.pvalue)
            print("  std err    :", reg.stderr)

            # Classify strength of the relationship
            if abs(reg.rvalue) >= 0.7:
                strength = "strong"
            elif abs(reg.rvalue) >= 0.4:
                strength = "moderate"
            else:
                strength = "weak"

            print("\nSimple interpretation:")
            print(f"- Relationship between {gap_col} and {delay_col} is {strength} "
                  f"(r = {reg.rvalue:.3f}).")
            if reg.pvalue < 0.05:
                print("- p-value < 0.05: slope is statistically significant "
                      "(unlikely to be random).")
            else:
                print("- p-value >= 0.05: slope is NOT statistically significant.")

            results["regression_gap_delay"] = {
                "slope": reg.slope,
                "intercept": reg.intercept,
                "rvalue": reg.rvalue,
                "r_squared": reg.rvalue ** 2,
                "pvalue": reg.pvalue,
                "stderr": reg.stderr,
            }

            # Regression plot (Figure 6)
            fig, ax = plt.subplots(figsize=(10, 7))
            fig.suptitle('Figure 6: Regression Analysis - Min Gap vs Min Delay',
                         fontsize=16, fontweight='bold', y=0.98)
            ax.scatter(x, y, alpha=0.6, s=50, color=EXEC_COLORS['primary'], edgecolors='white', linewidth=0.5)
            y_hat = reg.intercept + reg.slope * x
            ax.plot(x, y_hat, color=EXEC_COLORS['secondary'], linewidth=3, label='Regression Line', zorder=3)
            ax.set_xlabel(gap_col, fontsize=12, fontweight='bold')
            ax.set_ylabel(delay_col, fontsize=12, fontweight='bold')
            ax.set_title(f"{delay_col} vs {gap_col} (Linear Regression)", fontsize=14, fontweight='bold', pad=15)
            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
            ax.legend(frameon=True, fancybox=True, shadow=True, fontsize=10)
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            plt.tight_layout(pad=3.0)
            plt.show()

        else:
            print("[Warning] Not enough rows to run regression (need > 2).")
            results["regression_gap_delay"] = None
    else:
        print("[Warning] Min Gap or Min Delay column not found.")
        results["regression_gap_delay"] = None

    # 3.4.0 T-TEST: peak vs Off-Peak delay
    print("\n" + "-" * 60)
    print("3.4.0 Are Peak-Time Delays Worse? (Peak vs Off-Peak t-Test)")
    print("-" * 60)

    if (peak_col in df_cleaned.columns) and (delay_col in df_cleaned.columns):
        peak_flag = df_cleaned[peak_col]

        # If it's boolean True/False, convert to 1/0
        if peak_flag.dtype == "bool":
            peak_flag = peak_flag.astype(int)

        # Group 1: delays where is_peak == 1 (peak time)
        peak_delay = df_cleaned.loc[peak_flag == 1, delay_col].dropna()
        # Group 2: delays where is_peak == 0 (off-peak)
        off_delay = df_cleaned.loc[peak_flag == 0, delay_col].dropna()

        print(f"PEAK rows    : {len(peak_delay)}")
        print(f"OFF-PEAK rows: {len(off_delay)}")

        if (len(peak_delay) > 5) and (len(off_delay) > 5):
            # ttest_ind = two-sample t-test
            # equal_var=False = Welch's t-test (safer)
            t_stat, p_val = stats.ttest_ind(peak_delay, off_delay, equal_var=False)
            mean_peak = peak_delay.mean()
            mean_off = off_delay.mean()

            print("\nPeak vs Off-peak mean delay:")
            print(f"  mean_peak    = {mean_peak:.2f}")
            print(f"  mean_offpeak = {mean_off:.2f}")
            print(f"  t-statistic  = {t_stat:.3f}")
            print(f"  p-value      = {p_val:.4f}")

            print("\nInterpretation:")
            if p_val < 0.05:
                print("- p-value < 0.05: difference in mean delay is statistically significant.")
            else:
                print("- p-value >= 0.05: we do NOT have strong evidence of a difference.")

            results["ttest_peak_offpeak"] = {
                "mean_peak": mean_peak,
                "mean_offpeak": mean_off,
                "t_stat": t_stat,
                "pvalue": p_val,
            }
        else:
            print("[Warning] Not enough data for t-test (need > 5 in each group).")
            results["ttest_peak_offpeak"] = None
    else:
        print("[Warning] is_peak or Min Delay column not found in DataFrame.")
        results["ttest_peak_offpeak"] = None

    # Step 5.5) QUICK SUMMARY
    print("\n" + "-" * 60)
    print("5. Quick Summary")
    print("-" * 60)

    # Summary of correlation between Min Gap and Min Delay
    if (delay_col in num_cols) and (gap_col in num_cols):
        r = corr_numeric.loc[delay_col, gap_col]
        print(f"      Corr({delay_col}, {gap_col}) = {r:.3f}")
        if abs(r) > 0.4:
            print("  noticeable relationship between headway gap and delay.")
        else:
            print("   weak relationship between headway gap and delay.")

    # Summary of peak vs off-peak mean delays
    if results.get("ttest_peak_offpeak") is not None:
        mp = results["ttest_peak_offpeak"]["mean_peak"]
        mo = results["ttest_peak_offpeak"]["mean_offpeak"]
        print(f" Peak mean delay     ≈ {mp:.2f} minutes.")
        print(f" Off-peak mean delay ≈ {mo:.2f} minutes.")
        print("\n Min Gap is the strongest factor affecting delay duration,")
        print(" Peak vs off-peak delays are similar, with no meaningful difference,")
        print(" Specific lines (esp. BD (Bloor–Danforth)) experience higher counts of certain incident types,")

    # 3.5.0 Multivariate Relationships Visualization
    print("\n" + "-" * 60)
    print("3.5.0 Multivariate Relationships: Pair Plot")
    print("Figure 7: Pair plot matrix showing multivariate relationships between key variables")
    print("-" * 60)
    visualize_multivariate_relationships(df_cleaned)

    print("\n" + "=" * 60)
    print("Multivariate Relationships Summary")
    print("=" * 60)
    print("\nShows relationships between multiple variables using pair plots")

    print("\n" + "=" * 70)
    print("[Done] 3.0.0 Diagnostic analytics complete.")
    print("=" * 70 + "\n")

    return results

# ============================================================================
# STEP 5: PREDICTIVE ANALYTICS FUNCTIONS
# ============================================================================

def prepare_features_for_prediction(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
    """
    Prepare features and target variable for delay classification.

    Creates binary target: 1 for "long delay" (> median), 0 for "short delay" (≤ median).
    Encodes categorical features and selects relevant predictors.

    Args:
        df: Cleaned DataFrame with delay data

    Returns:
        Tuple of (X_features, y_target) where:
        - X_features: DataFrame with encoded features
        - y_target: Series with binary classification target (1=long, 0=short)
    """
    df_work = df.copy()

    # Create binary target: 1 = long delay (> median), 0 = short delay (≤ median)
    median_delay = df_work['min_delay'].median()
    df_work['is_long_delay'] = (df_work['min_delay'] > median_delay).astype(int)

    print(f"Median delay threshold: {median_delay:.2f} minutes")
    print(f"Short delays (≤ {median_delay:.2f} min): {(df_work['is_long_delay'] == 0).sum()} ({((df_work['is_long_delay'] == 0).sum() / len(df_work) * 100):.1f}%)")
    print(f"Long delays (> {median_delay:.2f} min): {(df_work['is_long_delay'] == 1).sum()} ({((df_work['is_long_delay'] == 1).sum() / len(df_work) * 100):.1f}%)")

    # Select features for prediction
    # Include: cause (code), hour, line, station, peak flags, min_gap, bound
    feature_cols = []

    # Encode categorical features
    le_code = LabelEncoder()
    le_line = LabelEncoder()
    le_station = LabelEncoder()
    le_bound = LabelEncoder()

    # Handle missing values in categorical columns before encoding
    df_work['code_encoded'] = le_code.fit_transform(df_work['code'].fillna('UNKNOWN'))
    df_work['line_encoded'] = le_line.fit_transform(df_work['line'].fillna('UNKNOWN'))
    df_work['station_encoded'] = le_station.fit_transform(df_work['station'].fillna('UNKNOWN'))
    df_work['bound_encoded'] = le_bound.fit_transform(df_work['bound'].fillna('UNKNOWN').astype(str))

    # Build feature matrix
    feature_cols = [
        'code_encoded',
        'line_encoded',
        'station_encoded',
        'bound_encoded',
        'hour',
        'month',
        'day_of_week',
        'is_peak',
        'is_morning_peak',
        'is_evening_peak',
        'min_gap',
        'bound_missing'
    ]

    # Only include columns that exist
    available_features = [col for col in feature_cols if col in df_work.columns]

    X = df_work[available_features].copy()
    y = df_work['is_long_delay'].copy()

    # Remove rows with any missing values in features
    mask = ~X.isnull().any(axis=1)
    X = X[mask]
    y = y[mask]

    print(f"\nFeatures used for prediction: {list(X.columns)}")
    print(f"Total samples: {len(X)}")

    return X, y


def train_and_evaluate_models(
    X_train: pd.DataFrame,
    X_test: pd.DataFrame,
    y_train: pd.Series,
    y_test: pd.Series
) -> Dict[str, Any]:
    """
    Train and evaluate three classification models.

    Models:
    1. Logistic Regression (baseline linear model)
    2. Decision Tree (interpretable, non-linear)
    3. Random Forest (ensemble, handles non-linearity well)

    Args:
        X_train: Training features
        X_test: Test features
        y_train: Training target
        y_test: Test target

    Returns:
        Dictionary containing:
        - models: Trained model objects
        - predictions: Test predictions for each model
        - metrics: Accuracy and F1 scores
        - feature_importance: Feature importance for tree-based models
    """
    results: Dict[str, Any] = {
        'models': {},
        'predictions': {},
        'metrics': {},
        'feature_importance': {}
    }

    print("\n" + "=" * 60)
    print("Training Classification Models")
    print("=" * 60)

    # Model 1: Logistic Regression
    print("\n1. Logistic Regression")
    print("-" * 60)

    # Convert is_long_delay to short_term_delay (1 = short delay, 0 = long delay)
    y_train_prepared = 1 - y_train.copy()

    # Use all available features (same as other models)
    feature_cols = list(X_train.columns)

    X_train_prepared = X_train[feature_cols].copy()

    # Convert boolean columns to integer (0 or 1)
    for col in X_train_prepared.select_dtypes(include='bool').columns:
        X_train_prepared.loc[:, col] = X_train_prepared.loc[:, col].astype(int)

    # Remove columns with zero variance (constant columns)
    constant_columns = X_train_prepared.columns[X_train_prepared.nunique() == 1]
    if not constant_columns.empty:
        X_train_prepared = X_train_prepared.drop(columns=constant_columns)

    # Scale features to prevent overflow and improve convergence
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train_scaled = pd.DataFrame(
        scaler.fit_transform(X_train_prepared),
        columns=X_train_prepared.columns,
        index=X_train_prepared.index
    )

    # Add constant term for intercept
    X_train_prepared = sm.add_constant(X_train_scaled, has_constant='add')

    # Create the logistic regression model
    logit_model = sm.Logit(y_train_prepared, X_train_prepared)

    # Fit the model using regularization to handle potential multicollinearity and improve convergence
    # Using L2 regularization with increased alpha and maxiter for better convergence
    try:
        import warnings
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            warnings.filterwarnings('ignore', category=UserWarning)
            # Use method='l1' with elastic_net_alpha=0.0 for L2 regularization
            # Increased alpha to 0.1 for stronger regularization to prevent overflow
            # Increased maxiter to 2000 for better convergence
            result = logit_model.fit_regularized(method='l1', alpha=0.1, elastic_net_alpha=0.0, maxiter=2000)

        # Display the model summary
        print("Logistic Regression Model Summary (Predicting short_term_delay):")
        print(result.summary())

        # Prepare test data and make predictions
        # Only use feature columns that exist in test data
        test_feature_cols = [col for col in feature_cols if col in X_test.columns]
        X_test_prepared = X_test[test_feature_cols].copy()
        for col in X_test_prepared.select_dtypes(include='bool').columns:
            X_test_prepared.loc[:, col] = X_test_prepared.loc[:, col].astype(int)
        constant_columns_test = X_test_prepared.columns[X_test_prepared.nunique() == 1]
        if not constant_columns_test.empty:
            X_test_prepared = X_test_prepared.drop(columns=constant_columns_test)

        # Scale test data using the same scaler from training
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test_prepared),
            columns=X_test_prepared.columns,
            index=X_test_prepared.index
        )
        X_test_prepared = sm.add_constant(X_test_scaled, has_constant='add')

        # Align columns with training data
        common_cols = list(set(X_train_prepared.columns) & set(X_test_prepared.columns))
        X_test_prepared = X_test_prepared[common_cols]

        # Make predictions and convert back to is_long_delay format
        lr_pred_proba = result.predict(X_test_prepared)
        lr_pred_short = (lr_pred_proba >= 0.5).astype(int)
        lr_pred = 1 - lr_pred_short  # Convert back to is_long_delay

        # Calculate metrics
        lr_acc = accuracy_score(y_test, lr_pred)
        lr_f1 = f1_score(y_test, lr_pred)

        print(f"\n   Accuracy: {lr_acc:.4f}")
        print(f"   F1-Score: {lr_f1:.4f}")

        results['models']['Logistic Regression'] = result
        results['predictions']['Logistic Regression'] = lr_pred
        results['metrics']['Logistic Regression'] = {'accuracy': lr_acc, 'f1': lr_f1}

    except np.linalg.LinAlgError as e:
        print(f"Caught LinAlgError during regularized fit: {e}")
        print("Consider adjusting 'alpha' or the 'method' for fit_regularized, or further feature selection.")
    except Exception as e:
        print(f"An unexpected error occurred during regularized fit: {e}")

    # Model 2: Decision Tree
    print("\n2. Decision Tree")
    print("-" * 60)
    dt_model = DecisionTreeClassifier(random_state=42, max_depth=10)
    dt_model.fit(X_train, y_train)
    dt_pred = dt_model.predict(X_test)
    dt_acc = accuracy_score(y_test, dt_pred)
    dt_f1 = f1_score(y_test, dt_pred)

    print(f"   Accuracy: {dt_acc:.4f}")
    print(f"   F1-Score: {dt_f1:.4f}")

    # Feature importance for Decision Tree
    dt_importance = pd.Series(dt_model.feature_importances_, index=X_train.columns)
    dt_importance = dt_importance.sort_values(ascending=False)
    print("\n   Top 5 Most Important Features:")
    for feat, imp in dt_importance.head(5).items():
        print(f"      {feat}: {imp:.4f}")

    results['models']['Decision Tree'] = dt_model
    results['predictions']['Decision Tree'] = dt_pred
    results['metrics']['Decision Tree'] = {'accuracy': dt_acc, 'f1': dt_f1}
    results['feature_importance']['Decision Tree'] = dt_importance

    # Model 3: Random Forest
    print("\n3. Random Forest")
    print("-" * 60)
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
    rf_model.fit(X_train, y_train)
    rf_pred = rf_model.predict(X_test)
    rf_acc = accuracy_score(y_test, rf_pred)
    rf_f1 = f1_score(y_test, rf_pred)

    print(f"   Accuracy: {rf_acc:.4f}")
    print(f"   F1-Score: {rf_f1:.4f}")

    # Feature importance for Random Forest
    rf_importance = pd.Series(rf_model.feature_importances_, index=X_train.columns)
    rf_importance = rf_importance.sort_values(ascending=False)
    print("\n   Top 5 Most Important Features:")
    for feat, imp in rf_importance.head(5).items():
        print(f"      {feat}: {imp:.4f}")

    results['models']['Random Forest'] = rf_model
    results['predictions']['Random Forest'] = rf_pred
    results['metrics']['Random Forest'] = {'accuracy': rf_acc, 'f1': rf_f1}
    results['feature_importance']['Random Forest'] = rf_importance

    # Model Comparison
    print("\n" + "=" * 60)
    print("Model Comparison Summary")
    print("=" * 60)
    comparison_df = pd.DataFrame(results['metrics']).T
    comparison_df = comparison_df.sort_values('f1', ascending=False)
    print(comparison_df)

    return results


def run_predictive_analytics(df_cleaned: pd.DataFrame) -> Dict[str, Any]:
    """
    Main function to run predictive analytics pipeline.

    Pipeline:
    1. Prepare features and create binary target (short vs long delay)
    2. Split data into 70/30 train/test sets
    3. Train three classification models
    4. Evaluate and compare models
    5. Analyze feature importance

    Args:
        df_cleaned: Cleaned DataFrame from data cleaning pipeline

    Returns:
        Dictionary containing all model results, metrics, and feature importance
    """
    print("\n" + "=" * 70)
    print("4.0.0 PREDICTIVE ANALYTICS: Short vs. Long Delay Classification")
    print("=" * 70)

    # 4.1.0 Prepare features and target
    print("\n" + "-" * 60)
    print("4.1.0 Preparing Features and Target Variable")
    print("-" * 60)
    X, y = prepare_features_for_prediction(df_cleaned)

    # 4.2.0 Train/Test Split (70/30)
    print("\n" + "-" * 60)
    print("4.2.0 Splitting Data (70% train, 30% test)")
    print("-" * 60)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    print(f"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)")
    print(f"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)")

    # 4.3.0 Train and evaluate models
    print("\n" + "-" * 60)
    print("4.3.0 Training and Evaluating Models")
    print("-" * 60)
    results = train_and_evaluate_models(X_train, X_test, y_train, y_test)

    # Store X_test and y_test for visualization
    results['X_test'] = X_test
    results['y_test'] = y_test

    # 4.4.0 Visualize Model Results
    print("\n" + "-" * 60)
    print("4.4.0 Visualizing Model Results")
    print("Figure 8: Model performance comparison, feature importance, and confusion matrices")
    print("-" * 60)
    visualize_predictive_model_results(results, X_test, y_test)

    # Model Performance Summary
    print("\n" + "-" * 60)
    print("Model Performance Summary")
    print("-" * 60)
    comparison_df = pd.DataFrame(results['metrics']).T
    best_model = comparison_df.sort_values('f1', ascending=False).index[0]
    best_accuracy = comparison_df.loc[best_model, 'accuracy']
    best_f1 = comparison_df.loc[best_model, 'f1']

    print(f"\nModel Performance Summary:")
    print(f"   • Best performing model: {best_model}")
    print(f"   • Accuracy: {best_accuracy:.1%}")
    print(f"   • F1-Score: {best_f1:.1%}")

    # 4.5.0 Feature Importance Analysis
    print("\n" + "-" * 60)
    print("4.5.0 Feature Importance Analysis")
    print("-" * 60)
    print("\nWhich factors are most predictive of long delays?")

    # Use Random Forest feature importance (most reliable)
    if 'Random Forest' in results['feature_importance']:
        rf_imp = results['feature_importance']['Random Forest']
        print("\nTop 10 Most Important Features (Random Forest):")
        for i, (feat, imp) in enumerate(rf_imp.head(10).items(), 1):
            print(f"  {i:2d}. {feat:20s}: {imp:.4f}")

        # Feature Importance Summary
        print("\n" + "-" * 60)
        print("Feature Importance Summary")
        print("-" * 60)
        top_feature = rf_imp.index[0]
        top_importance = rf_imp.iloc[0]
        print(f"   • Most important factor: '{top_feature}' (importance: {top_importance:.1%})")

    print("\n" + "=" * 70)
    print("[Done] 4.0.0 Predictive analytics complete.")
    print("=" * 70 + "\n")

    return results

# ============================================================================
# STEP 6: VISUALIZATION FUNCTIONS
# ============================================================================
"""**Visualization Categories:**
1. **Data Quality**: Before/after cleaning comparisons
2. **Distribution Analysis**: Histograms, box plots showing delay patterns
3. **Relationship Analysis**: Correlation heatmaps, scatter plots
4. **Trend Analysis**: Time series showing delays over months/days
5. **Comparative Analysis**: Bar charts, pie charts comparing groups
6. **Predictive Model Results**: Confusion matrices, feature importance, model comparison
7. **Multivariate Analysis**: Pair plots for key relationships
"""

def visualize_data_quality_before_after(df_before: pd.DataFrame, df_after: pd.DataFrame) -> None:
    """
    Visualize data quality improvements from cleaning process.

    Figure 1: Data Quality - Before vs After Cleaning Comparison

    Creates before/after comparisons for:
    - Total records
    - Missing values
    - Data completeness

    Args:
        df_before: DataFrame before cleaning (raw data)
        df_after: DataFrame after cleaning
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    fig.suptitle('Figure 1: Data Quality - Before vs After Cleaning', fontsize=16, fontweight='bold', y=1.02)

    # Chart 1: Record count comparison
    categories = ['Before Cleaning', 'After Cleaning']
    counts = [len(df_before), len(df_after)]
    colors = [EXEC_COLORS['primary'], EXEC_COLORS['secondary']]

    bars1 = axes[0].bar(categories, counts, color=colors, alpha=0.85, edgecolor='white', linewidth=2)
    axes[0].set_ylabel('Number of Records', fontsize=12, fontweight='bold')
    axes[0].set_title('Data Volume Comparison', fontsize=13, fontweight='bold', pad=15)
    axes[0].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
    axes[0].spines['top'].set_visible(False)
    axes[0].spines['right'].set_visible(False)

    # Add value labels on bars
    for i, (bar, count) in enumerate(zip(bars1, counts)):
        axes[0].text(bar.get_x() + bar.get_width()/2., count + max(counts)*0.02,
                     f'{count:,}', ha='center', va='bottom', fontsize=11, fontweight='bold')

    # Chart 2: Missing values comparison
    missing_before = df_before.isnull().sum().sum()
    missing_after = df_after.isnull().sum().sum()
    missing_data = [missing_before, missing_after]

    bars2 = axes[1].bar(categories, missing_data, color=colors, alpha=0.85, edgecolor='white', linewidth=2)
    axes[1].set_ylabel('Total Missing Values', fontsize=12, fontweight='bold')
    axes[1].set_title('Data Completeness Improvement', fontsize=13, fontweight='bold', pad=15)
    axes[1].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
    axes[1].spines['top'].set_visible(False)
    axes[1].spines['right'].set_visible(False)

    # Add value labels
    for i, (bar, val) in enumerate(zip(bars2, missing_data)):
        axes[1].text(bar.get_x() + bar.get_width()/2., val + max(missing_data)*0.02,
                     f'{val:,}', ha='center', va='bottom', fontsize=11, fontweight='bold')

    plt.tight_layout(pad=3.0)
    plt.show()

    # Print summary
    retention_rate = (len(df_after) / len(df_before)) * 100
    print(f"Data Retention Rate: {retention_rate:.2f}%")
    print(f"Records Removed: {len(df_before) - len(df_after):,} ({(100-retention_rate):.2f}%)")

    # Summary
    print("\n" + "=" * 60)
    print("Data Quality Summary")
    print("=" * 60)


def visualize_delay_distributions(df: pd.DataFrame) -> None:
    """
    Create histograms and box plots showing delay distributions.

    Visualizations:
    - Histogram of delay duration
    - Box plots by line, weekday, peak/off-peak

    Args:
        df: Cleaned DataFrame
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 11))
    fig.suptitle('Figure 2: Delay Distribution Analysis', fontsize=16, fontweight='bold', y=0.995)

    # 1. Histogram of delay duration
    axes[0, 0].hist(df['min_delay'], bins=30, color=EXEC_COLORS['primary'], alpha=0.75,
                    edgecolor='white', linewidth=1.5)
    axes[0, 0].axvline(df['min_delay'].median(), color=EXEC_COLORS['warning'],
                       linestyle='--', linewidth=2.5, label=f'Median: {df["min_delay"].median():.1f} min')
    axes[0, 0].axvline(df['min_delay'].mean(), color=EXEC_COLORS['accent'],
                       linestyle='--', linewidth=2.5, label=f'Mean: {df["min_delay"].mean():.1f} min')
    axes[0, 0].set_xlabel('Delay Duration (minutes)', fontsize=12, fontweight='bold')
    axes[0, 0].set_ylabel('Frequency', fontsize=12, fontweight='bold')
    axes[0, 0].set_title('Distribution of Delay Durations', fontsize=13, fontweight='bold', pad=10)
    axes[0, 0].legend(frameon=True, fancybox=True, shadow=True, fontsize=10)
    axes[0, 0].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
    axes[0, 0].spines['top'].set_visible(False)
    axes[0, 0].spines['right'].set_visible(False)

    # 2. Box plot by Line
    line_order = ['YU', 'BD', 'SHP']
    df_line = df[df['line'].isin(line_order)]
    box_data = [df_line[df_line['line'] == line]['min_delay'].values for line in line_order]
    bp = axes[0, 1].boxplot(box_data, tick_labels=line_order, patch_artist=True)
    line_colors = [EXEC_COLORS['primary'], EXEC_COLORS['secondary'], EXEC_COLORS['accent']]
    for i, patch in enumerate(bp['boxes']):
        patch.set_facecolor(line_colors[i % len(line_colors)])
        patch.set_alpha(0.8)
        patch.set_edgecolor('white')
        patch.set_linewidth(1.5)
    for median in bp['medians']:
        median.set_color(EXEC_COLORS['warning'])
        median.set_linewidth(2.5)
    axes[0, 1].set_ylabel('Delay Duration (minutes)', fontsize=12, fontweight='bold')
    axes[0, 1].set_xlabel('Subway Line', fontsize=12, fontweight='bold')
    axes[0, 1].set_title('Delay Distribution by Subway Line', fontsize=13, fontweight='bold', pad=10)
    axes[0, 1].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
    axes[0, 1].spines['top'].set_visible(False)
    axes[0, 1].spines['right'].set_visible(False)

    # 3. Box plot by Peak vs Off-Peak
    peak_data = [df[df['is_peak'] == 0]['min_delay'].values,
                 df[df['is_peak'] == 1]['min_delay'].values]
    bp = axes[1, 0].boxplot(peak_data, tick_labels=['Off-Peak', 'Peak'], patch_artist=True)
    peak_colors = [EXEC_COLORS['neutral'], EXEC_COLORS['accent']]
    for i, patch in enumerate(bp['boxes']):
        patch.set_facecolor(peak_colors[i])
        patch.set_alpha(0.8)
        patch.set_edgecolor('white')
        patch.set_linewidth(1.5)
    for median in bp['medians']:
        median.set_color(EXEC_COLORS['warning'])
        median.set_linewidth(2.5)
    axes[1, 0].set_ylabel('Delay Duration (minutes)', fontsize=12, fontweight='bold')
    axes[1, 0].set_xlabel('Time Period', fontsize=12, fontweight='bold')
    axes[1, 0].set_title('Delay Distribution: Peak vs Off-Peak Hours', fontsize=13, fontweight='bold', pad=10)
    axes[1, 0].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
    axes[1, 0].spines['top'].set_visible(False)
    axes[1, 0].spines['right'].set_visible(False)

    # 4. Box plot by Weekday
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    weekday_data = [df[df['weekday'] == day]['min_delay'].values for day in weekday_order]
    bp = axes[1, 1].boxplot(weekday_data, tick_labels=[d[:3] for d in weekday_order], patch_artist=True)
    for patch in bp['boxes']:
        patch.set_facecolor(EXEC_COLORS['secondary'])
        patch.set_alpha(0.8)
        patch.set_edgecolor('white')
        patch.set_linewidth(1.5)
    for median in bp['medians']:
        median.set_color(EXEC_COLORS['warning'])
        median.set_linewidth(2.5)
    axes[1, 1].set_ylabel('Delay Duration (minutes)', fontsize=12, fontweight='bold')
    axes[1, 1].set_xlabel('Day of Week', fontsize=12, fontweight='bold')
    axes[1, 1].set_title('Delay Distribution by Day of Week', fontsize=13, fontweight='bold', pad=10)
    axes[1, 1].tick_params(axis='x', rotation=45)
    axes[1, 1].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
    axes[1, 1].spines['top'].set_visible(False)
    axes[1, 1].spines['right'].set_visible(False)

    plt.tight_layout(pad=3.0)
    plt.show()

    # Summary
    print("\n" + "=" * 60)
    print("Delay Distribution Summary")
    print("=" * 60)
    print("Shows delay patterns by line, time of day, and day of week")


def visualize_correlation_heatmap(df: pd.DataFrame) -> None:
    """
    Create correlation heatmap for numeric variables.

    Figure 5: Correlation Analysis Heatmap

    Args:
        df: Cleaned DataFrame
    """
    # Select numeric columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    # Remove ID columns and flags
    exclude_cols = ['_id', 'bound_missing']
    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]

    # Calculate correlation matrix
    corr_matrix = df[numeric_cols].corr()

    # Create heatmap using matplotlib
    fig, ax = plt.subplots(figsize=(14, 11))
    fig.suptitle('Figure 5: Correlation Analysis - Relationships Between Key Variables',
                 fontsize=16, fontweight='bold', y=0.98)

    # Use a professional colormap
    im = ax.imshow(corr_matrix.values, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)

    # Set ticks and labels
    ax.set_xticks(np.arange(len(corr_matrix.columns)))
    ax.set_yticks(np.arange(len(corr_matrix.columns)))
    ax.set_xticklabels(corr_matrix.columns, rotation=45, ha='right', fontsize=10, fontweight='bold')
    ax.set_yticklabels(corr_matrix.columns, fontsize=10, fontweight='bold')

    # Add text annotations with better styling
    for i in range(len(corr_matrix.columns)):
        for j in range(len(corr_matrix.columns)):
            if i <= j:  # Only show lower triangle
                value = corr_matrix.iloc[i, j]
                text_color = 'white' if abs(value) > 0.5 else 'black'
                text_weight = 'bold' if abs(value) > 0.5 else 'normal'
                ax.text(j, i, f'{value:.2f}',
                       ha="center", va="center", color=text_color,
                       fontsize=10, fontweight=text_weight)

    # Add colorbar with professional styling
    cbar = plt.colorbar(im, ax=ax, shrink=0.75, pad=0.02)
    cbar.set_label('Correlation Coefficient', rotation=270, labelpad=25,
                   fontsize=12, fontweight='bold')
    cbar.ax.tick_params(labelsize=10)

    ax.set_title('Variable Relationships Matrix', fontsize=14, fontweight='bold', pad=20)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.spines['left'].set_visible(False)

    plt.tight_layout(pad=3.0)
    plt.show()

    # Summary
    print("\n" + "=" * 60)
    print("Correlation Summary")
    print("=" * 60)
    print("Shows relationships between variables using correlation heatmap")


def visualize_trends_over_time(df: pd.DataFrame) -> None:
    """
    Create time series visualizations showing delay trends.

    Figure 4: Trends Over Time Analysis

    Visualizations:
    - Monthly average delays
    - Daily delay counts
    - Hourly delay patterns

    Args:
        df: Cleaned DataFrame with datetime column
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 11))
    fig.suptitle('Figure 4: Delay Trends Over Time', fontsize=16, fontweight='bold', y=0.995)

    # 1. Monthly average delays
    monthly_avg = df.groupby('month')['min_delay'].mean().sort_index()
    axes[0, 0].plot(monthly_avg.index, monthly_avg.values, marker='o', linewidth=3,
                    color=EXEC_COLORS['primary'], markersize=8, markerfacecolor='white',
                    markeredgewidth=2, markeredgecolor=EXEC_COLORS['primary'])
    axes[0, 0].fill_between(monthly_avg.index, monthly_avg.values, alpha=0.25,
                            color=EXEC_COLORS['light_blue'])
    axes[0, 0].set_xlabel('Month', fontsize=12, fontweight='bold')
    axes[0, 0].set_ylabel('Average Delay (minutes)', fontsize=12, fontweight='bold')
    axes[0, 0].set_title('Average Delay Duration by Month', fontsize=13, fontweight='bold', pad=10)
    axes[0, 0].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
    axes[0, 0].set_xticks(range(1, 13))
    axes[0, 0].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], fontsize=10)
    axes[0, 0].spines['top'].set_visible(False)
    axes[0, 0].spines['right'].set_visible(False)

    # 2. Monthly delay counts
    monthly_counts = df.groupby('month').size().sort_index()
    bars = axes[0, 1].bar(monthly_counts.index, monthly_counts.values,
                         color=EXEC_COLORS['primary'], alpha=0.85, edgecolor='white', linewidth=1.5)
    axes[0, 1].set_xlabel('Month', fontsize=12, fontweight='bold')
    axes[0, 1].set_ylabel('Number of Delays', fontsize=12, fontweight='bold')
    axes[0, 1].set_title('Total Delay Incidents by Month', fontsize=13, fontweight='bold', pad=10)
    axes[0, 1].grid(True, alpha=0.3, linestyle='--', linewidth=0.8, axis='y')
    axes[0, 1].set_xticks(range(1, 13))
    axes[0, 1].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],
                               rotation=45, ha='right', fontsize=10)
    axes[0, 1].spines['top'].set_visible(False)
    axes[0, 1].spines['right'].set_visible(False)

    # 3. Hourly delay patterns
    hourly_avg = df.groupby('hour')['min_delay'].mean().sort_index()
    axes[1, 0].plot(hourly_avg.index, hourly_avg.values, marker='s', linewidth=3,
                    color=EXEC_COLORS['secondary'], markersize=6, markerfacecolor='white',
                    markeredgewidth=2, markeredgecolor=EXEC_COLORS['secondary'])
    axes[1, 0].axvspan(7, 10, alpha=0.15, color=EXEC_COLORS['accent'], label='Morning Peak')
    axes[1, 0].axvspan(16, 19, alpha=0.15, color=EXEC_COLORS['warning'], label='Evening Peak')
    axes[1, 0].set_xlabel('Hour of Day', fontsize=12, fontweight='bold')
    axes[1, 0].set_ylabel('Average Delay (minutes)', fontsize=12, fontweight='bold')
    axes[1, 0].set_title('Average Delay by Hour of Day', fontsize=13, fontweight='bold', pad=10)
    axes[1, 0].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
    axes[1, 0].legend(frameon=True, fancybox=True, shadow=True, fontsize=10)
    axes[1, 0].set_xticks(range(0, 24, 2))
    axes[1, 0].spines['top'].set_visible(False)
    axes[1, 0].spines['right'].set_visible(False)

    # 4. Delay counts by weekday
    weekday_counts = df.groupby('weekday').size()
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    weekday_counts = weekday_counts.reindex(weekday_order)
    colors = [EXEC_COLORS['secondary'] if day in ['Saturday', 'Sunday'] else EXEC_COLORS['primary']
              for day in weekday_order]
    bars = axes[1, 1].bar(range(len(weekday_order)), weekday_counts.values,
                         color=colors, alpha=0.85, edgecolor='white', linewidth=1.5)
    axes[1, 1].set_xlabel('Day of Week', fontsize=12, fontweight='bold')
    axes[1, 1].set_ylabel('Number of Delays', fontsize=12, fontweight='bold')
    axes[1, 1].set_title('Total Delay Incidents by Day of Week', fontsize=13, fontweight='bold', pad=10)
    axes[1, 1].set_xticks(range(len(weekday_order)))
    axes[1, 1].set_xticklabels([d[:3] for d in weekday_order], rotation=45, ha='right', fontsize=10)
    axes[1, 1].grid(True, alpha=0.3, linestyle='--', linewidth=0.8, axis='y')
    axes[1, 1].spines['top'].set_visible(False)
    axes[1, 1].spines['right'].set_visible(False)

    plt.tight_layout(pad=3.0)
    plt.show()

    # Summary
    print("\n" + "=" * 60)
    print("Trends Over Time Summary")
    print("=" * 60)
    print("Shows delay patterns by month, hour, and day of week")


def visualize_comparative_analysis(df: pd.DataFrame) -> None:
    """
    Create bar charts and pie charts comparing delays across categories.

    Visualizations:
    - Delays by line (bar chart)
    - Top 10 stations by delay count (bar chart)
    - Top 10 delay codes (bar chart)
    - Delay distribution by line (pie chart)

    Args:
        df: Cleaned DataFrame
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 11))
    fig.suptitle('Figure 3: Comparative Analysis - Delays Across Categories', fontsize=16, fontweight='bold', y=0.995)

    # 1. Average delay by line
    line_avg = df.groupby('line')['min_delay'].mean().sort_values(ascending=False)
    line_colors = [EXEC_COLORS['primary'], EXEC_COLORS['secondary'], EXEC_COLORS['accent']]
    bars = axes[0, 0].bar(line_avg.index, line_avg.values,
                          color=line_colors[:len(line_avg)], alpha=0.85, edgecolor='white', linewidth=2)
    axes[0, 0].set_ylabel('Average Delay (minutes)', fontsize=12, fontweight='bold')
    axes[0, 0].set_xlabel('Subway Line', fontsize=12, fontweight='bold')
    axes[0, 0].set_title('Average Delay Duration by Line', fontsize=13, fontweight='bold', pad=10)
    axes[0, 0].grid(True, alpha=0.3, linestyle='--', linewidth=0.8, axis='y')
    axes[0, 0].spines['top'].set_visible(False)
    axes[0, 0].spines['right'].set_visible(False)
    for bar in bars:
        height = bar.get_height()
        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + max(line_avg.values)*0.02,
                f'{height:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

    # 2. Top 10 stations by delay count
    top_stations = df['station'].value_counts().head(10)
    bars = axes[0, 1].barh(range(len(top_stations)), top_stations.values,
                          color=EXEC_COLORS['secondary'], alpha=0.85, edgecolor='white', linewidth=1.5)
    axes[0, 1].set_yticks(range(len(top_stations)))
    axes[0, 1].set_yticklabels([s[:25] + '...' if len(s) > 25 else s for s in top_stations.index],
                               fontsize=9, fontweight='bold')
    axes[0, 1].set_xlabel('Number of Delays', fontsize=12, fontweight='bold')
    axes[0, 1].set_title('Top 10 Stations by Delay Count', fontsize=13, fontweight='bold', pad=10)
    axes[0, 1].grid(True, alpha=0.3, linestyle='--', linewidth=0.8, axis='x')
    axes[0, 1].invert_yaxis()
    axes[0, 1].spines['top'].set_visible(False)
    axes[0, 1].spines['right'].set_visible(False)
    for i, (idx, val) in enumerate(zip(top_stations.index, top_stations.values)):
        axes[0, 1].text(val + max(top_stations.values)*0.01, i, f'{val:,}',
                va='center', fontsize=9, fontweight='bold')

    # 3. Top 10 delay codes
    top_codes = df['code'].value_counts().head(10)
    bars = axes[1, 0].bar(range(len(top_codes)), top_codes.values,
                         color=EXEC_COLORS['accent'], alpha=0.85, edgecolor='white', linewidth=1.5)
    axes[1, 0].set_xticks(range(len(top_codes)))
    axes[1, 0].set_xticklabels(top_codes.index, rotation=45, ha='right', fontsize=9, fontweight='bold')
    axes[1, 0].set_ylabel('Number of Delays', fontsize=12, fontweight='bold')
    axes[1, 0].set_xlabel('Delay Code', fontsize=12, fontweight='bold')
    axes[1, 0].set_title('Top 10 Delay Codes by Frequency', fontsize=13, fontweight='bold', pad=10)
    axes[1, 0].grid(True, alpha=0.3, linestyle='--', linewidth=0.8, axis='y')
    axes[1, 0].spines['top'].set_visible(False)
    axes[1, 0].spines['right'].set_visible(False)
    for bar in bars:
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + max(top_codes.values)*0.01,
                f'{int(height):,}', ha='center', va='bottom', fontsize=9, fontweight='bold')

    # 4. Pie chart: Delay distribution by line
    line_counts = df['line'].value_counts()
    colors_pie = [EXEC_COLORS['primary'], EXEC_COLORS['secondary'], EXEC_COLORS['accent']]
    wedges, texts, autotexts = axes[1, 1].pie(line_counts.values, labels=line_counts.index,
                                        autopct='%1.1f%%', colors=colors_pie[:len(line_counts)],
                                        startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'},
                                        wedgeprops={'edgecolor': 'white', 'linewidth': 2})
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(11)
    axes[1, 1].set_title('Delay Distribution by Subway Line', fontsize=13, fontweight='bold', pad=15)

    plt.tight_layout(pad=3.0)
    plt.show()

    # Summary
    print("\n" + "=" * 60)
    print("Comparative Analysis Summary")
    print("=" * 60)
    print("Compares delays across lines, stations, and delay causes")


def visualize_predictive_model_results(results: Dict[str, Any], X_test: pd.DataFrame, y_test: pd.Series) -> None:
    """
    Visualize predictive model performance and insights.

    Figure 8: Predictive Model Performance and Results

    Visualizations:
    - Model comparison (accuracy/F1 bar chart)
    - Feature importance (bar chart)
    - Confusion matrices for each model

    Args:
        results: Dictionary from run_predictive_analytics containing model results
        X_test: Test features
        y_test: Test target
    """
    from sklearn.metrics import confusion_matrix

    fig, axes = plt.subplots(2, 3, figsize=(16, 11))
    fig.suptitle('Figure 8: Predictive Model Performance Analysis', fontsize=16, fontweight='bold', y=0.995)

    # 1. Model comparison bar chart
    metrics_df = pd.DataFrame(results['metrics']).T
    x = np.arange(len(metrics_df))
    width = 0.35
    bars1 = axes[0, 0].bar(x - width/2, metrics_df['accuracy'], width, label='Accuracy',
                    color=EXEC_COLORS['primary'], alpha=0.85, edgecolor='white', linewidth=1.5)
    bars2 = axes[0, 0].bar(x + width/2, metrics_df['f1'], width, label='F1-Score',
                    color=EXEC_COLORS['accent'], alpha=0.85, edgecolor='white', linewidth=1.5)
    axes[0, 0].set_ylabel('Score', fontsize=12, fontweight='bold')
    axes[0, 0].set_xlabel('Model', fontsize=12, fontweight='bold')
    axes[0, 0].set_title('Model Performance Comparison', fontsize=13, fontweight='bold', pad=10)
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(metrics_df.index, rotation=15, ha='right', fontsize=10, fontweight='bold')
    axes[0, 0].legend(frameon=True, fancybox=True, shadow=True, fontsize=10, loc='lower right')
    axes[0, 0].grid(True, alpha=0.3, linestyle='--', linewidth=0.8, axis='y')
    axes[0, 0].set_ylim([0, 1.05])
    axes[0, 0].spines['top'].set_visible(False)
    axes[0, 0].spines['right'].set_visible(False)
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.015,
                    f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

    # 2. Feature importance (Random Forest, fallback to Decision Tree)
    feature_imp = None
    model_name_imp = None

    if 'Random Forest' in results.get('feature_importance', {}) and len(results['feature_importance']['Random Forest']) > 0:
        feature_imp = results['feature_importance']['Random Forest'].head(10)
        model_name_imp = 'Random Forest'
    elif 'Decision Tree' in results.get('feature_importance', {}) and len(results['feature_importance']['Decision Tree']) > 0:
        feature_imp = results['feature_importance']['Decision Tree'].head(10)
        model_name_imp = 'Decision Tree'

    if feature_imp is not None and len(feature_imp) > 0:
        bars = axes[0, 1].barh(range(len(feature_imp)), feature_imp.values,
                              color=EXEC_COLORS['success'], alpha=0.85, edgecolor='white', linewidth=1.5)
        axes[0, 1].set_yticks(range(len(feature_imp)))
        axes[0, 1].set_yticklabels(feature_imp.index, fontsize=10, fontweight='bold')
        axes[0, 1].set_xlabel('Importance Score', fontsize=12, fontweight='bold')
        axes[0, 1].set_title(f'Top 10 Most Important Features ({model_name_imp})',
                            fontsize=13, fontweight='bold', pad=10)
        axes[0, 1].grid(True, alpha=0.3, linestyle='--', linewidth=0.8, axis='x')
        axes[0, 1].invert_yaxis()
        axes[0, 1].spines['top'].set_visible(False)
        axes[0, 1].spines['right'].set_visible(False)
        if max(feature_imp.values) > 0:
            for i, (idx, val) in enumerate(zip(feature_imp.index, feature_imp.values)):
                axes[0, 1].text(val + max(feature_imp.values)*0.01, i, f'{val:.4f}',
                        va='center', fontsize=9, fontweight='bold')
    else:
        axes[0, 1].text(0.5, 0.5, 'Feature importance\nnot available',
                       ha='center', va='center', fontsize=12,
                       transform=axes[0, 1].transAxes)
        axes[0, 1].set_title('Feature Importance')
        axes[0, 1].axis('off')

    # 3-5. Confusion matrices
    model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest']
    for idx, model_name in enumerate(model_names):
        if model_name in results.get('predictions', {}):
            y_pred = results['predictions'][model_name]
            if len(y_pred) > 0:
                cm = confusion_matrix(y_test, y_pred)
                im = axes[1, idx].imshow(cm, cmap='Blues', aspect='auto', vmin=0, vmax=cm.max())
                axes[1, idx].set_xticks([0, 1])
                axes[1, idx].set_yticks([0, 1])
                axes[1, idx].set_xticklabels(['Short', 'Long'], fontsize=11, fontweight='bold')
                axes[1, idx].set_yticklabels(['Short', 'Long'], fontsize=11, fontweight='bold')
                axes[1, idx].set_xlabel('Predicted', fontsize=12, fontweight='bold')
                axes[1, idx].set_ylabel('Actual', fontsize=12, fontweight='bold')
                axes[1, idx].set_title(f'{model_name}\nConfusion Matrix',
                                       fontsize=12, fontweight='bold', pad=10)
                # Add text annotations with labels
                # Define labels for each cell: [True Negative, False Positive], [False Negative, True Positive]
                labels = [['True Negative', 'False Positive'], ['False Negative', 'True Positive']]
                for i in range(2):
                    for j in range(2):
                        # Add count number
                        axes[1, idx].text(j, i-0.18, str(cm[i, j]), ha='center', va='center',
                                         color='white' if cm[i, j] > cm.max()/2 else 'black',
                                         fontweight='bold', fontsize=13)
                        # Add label below the count
                        axes[1, idx].text(j, i+0.18, labels[i][j], ha='center', va='center',
                                         color='white' if cm[i, j] > cm.max()/2 else 'black',
                                         fontsize=9, style='italic', fontweight='normal')
                axes[1, idx].spines['top'].set_visible(False)
                axes[1, idx].spines['right'].set_visible(False)
                axes[1, idx].spines['bottom'].set_visible(False)
                axes[1, idx].spines['left'].set_visible(False)
            else:
                axes[1, idx].text(0.5, 0.5, 'No predictions\navailable',
                                 ha='center', va='center', fontsize=12,
                                 transform=axes[1, idx].transAxes)
                axes[1, idx].set_title(f'{model_name}\nConfusion Matrix')
                axes[1, idx].axis('off')
        else:
            axes[1, idx].text(0.5, 0.5, f'{model_name}\nnot available',
                             ha='center', va='center', fontsize=12,
                             transform=axes[1, idx].transAxes)
            axes[1, idx].set_title(f'{model_name}\nConfusion Matrix')
            axes[1, idx].axis('off')

    plt.tight_layout(pad=3.0)
    plt.show()

    # Summary
    print("\n" + "=" * 60)
    print("Predictive Model Performance Summary")
    print("=" * 60)
    print("Shows model accuracy, feature importance, and prediction results")


def visualize_multivariate_relationships(df: pd.DataFrame) -> None:
    """
    Create scatter plot matrix showing relationships between key numeric variables.

    Figure 7: Multivariate Relationships - Pair Plot Analysis

    Args:
        df: Cleaned DataFrame
    """
    # Select key numeric variables
    key_vars = ['min_delay', 'min_gap', 'hour', 'month']
    key_vars = [var for var in key_vars if var in df.columns]

    if len(key_vars) >= 2:
        # Sample data
        pair_df = df[key_vars].sample(min(1000, len(df)))

        # Create scatter plot matrix using matplotlib
        n_vars = len(key_vars)
        fig, axes = plt.subplots(n_vars, n_vars, figsize=(14, 13))
        fig.suptitle('Figure 7: Multivariate Relationships - Pair Plot Analysis',
                     fontsize=16, fontweight='bold', y=0.995)

        for i in range(n_vars):
            for j in range(n_vars):
                if i == j:
                    # Diagonal: histogram
                    axes[i, j].hist(pair_df[key_vars[i]], bins=30, alpha=0.75,
                                   color=EXEC_COLORS['primary'], edgecolor='white', linewidth=1)
                    axes[i, j].set_ylabel('Frequency', fontsize=10, fontweight='bold')
                    axes[i, j].spines['top'].set_visible(False)
                    axes[i, j].spines['right'].set_visible(False)
                    axes[i, j].grid(True, alpha=0.3, linestyle='--', linewidth=0.8, axis='y')
                else:
                    # Off-diagonal: scatter plot
                    axes[i, j].scatter(pair_df[key_vars[j]], pair_df[key_vars[i]],
                                      alpha=0.5, s=25, color=EXEC_COLORS['primary'],
                                      edgecolors='white', linewidth=0.3)
                    axes[i, j].spines['top'].set_visible(False)
                    axes[i, j].spines['right'].set_visible(False)
                    axes[i, j].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)

                # Set labels only on edges
                if i == n_vars - 1:
                    axes[i, j].set_xlabel(key_vars[j], fontsize=11, fontweight='bold')
                if j == 0:
                    axes[i, j].set_ylabel(key_vars[i], fontsize=11, fontweight='bold')

        plt.tight_layout(pad=3.0)
        plt.show()

        # Summary
        print("\n" + "=" * 60)
        print("Multivariate Relationships Summary")
        print("=" * 60)
        print("Shows relationships between multiple variables using pair plots")


def run_comprehensive_visualizations(
    df_cleaned: pd.DataFrame,
    df_before: Optional[pd.DataFrame] = None,
    results_predictive: Optional[Dict[str, Any]] = None
) -> None:
    """
    Optional function to run additional visualizations.

    Note: Most visualizations are now integrated into their respective sections:
    - Data Cleaning: visualize_data_quality_before_after()
    - Descriptive Analytics: visualize_delay_distributions(), visualize_comparative_analysis(), visualize_trends_over_time()
    - Diagnostic Analytics: visualize_correlation_heatmap(), regression plot, visualize_multivariate_relationships()
    - Predictive Analytics: visualize_predictive_model_results()

    This function can be used to optionally show predictive model results if not already shown.

    Args:
        df_cleaned: Cleaned DataFrame
        df_before: Optional raw DataFrame for before/after comparison (not used here, already shown in cleaning section)
        results_predictive: Optional predictive analytics results
    """
    print("\n" + "=" * 60)
    print("ADDITIONAL VISUALIZATIONS")
    print("=" * 60)
    print("Note: Most visualizations are now integrated into their respective analysis sections.")
    print("=" * 60)

    # Optional: Show predictive model results if provided and not already shown
    if results_predictive is not None:
        print("\nPredictive Model Results: Logistic Regression, Decision Tree, and Random Forest")
        print("-" * 60)
        # Use the full visualization function if X_test and y_test are available
        if 'X_test' in results_predictive and 'y_test' in results_predictive:
            visualize_predictive_model_results(
                results_predictive,
                results_predictive['X_test'],
                results_predictive['y_test']
            )
        else:
            print("Note: Full visualizations require test data (X_test, y_test) in results_predictive.")
    else:
        print("\nNo additional visualizations to display.")
        print("All visualizations are integrated into their respective analysis sections.")

    print("\n" + "=" * 60)
    print("[Done] Additional visualizations complete.")
    print("=" * 60 + "\n")

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main() -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
    """
    Main execution function that orchestrates the entire analysis pipeline.

    Pipeline steps:
    1. Display TTC code categories reference
    2. Load 2024 and 2025 datasets
    3. Inspect raw datasets
    4. Clean both datasets
    5. Combine cleaned datasets
    6. Display data quality summary
    7. Run descriptive analytics on cleaned data
    8. Run diagnostic analytics on cleaned data

    Returns:
        Tuple containing:
        - df_final: Final cleaned and combined DataFrame (or None if error)
        - df_original: Original combined DataFrame before cleaning (or None if error)
        - results_desc: Descriptive analytics results dictionary (or None if error)
        - results_diag: Diagnostic analytics results dictionary (or None if error)
    """

    # ============================================================================
    # 1.0.0 DATA PREPARATION
    # ============================================================================
    print("\n" + "=" * 70)
    print("1.0.0 DATA PREPARATION")
    print("=" * 70)

    # 1.1.0 Load and inspect the dataset
    print("\n" + "-" * 60)
    print("1.1.0 Load and Inspect the Dataset")
    print("-" * 60)
    df_2024 = load_ttc_data("ttc-subway-delay-data-2024.csv")
    df_2025 = load_ttc_data("TTC Subway Delay Data since 2025.csv")

    # 1.1.1 Ensuring you understand its structure and data types
    if df_2024 is not None:
        print("\n" + "-" * 50)
        print("1.1.1 2024 Dataset Inspection - Understanding Structure and Data Types")
        print("-" * 50)
        inspect_dataframe(df_2024, "2024 Dataset")

    if df_2025 is not None:
        print("\n" + "-" * 50)
        print("1.1.1 2025 Dataset Inspection - Understanding Structure and Data Types")
        print("-" * 50)
        inspect_dataframe(df_2025, "2025 Dataset")

    # 1.2.0 Identify and handle missing data
    # 1.3.0 Detect and handle outliers
    # 1.4.0 Resolve data inconsistencies
    # 1.5.0 Transform and normalize variables
    print("\n" + "-" * 60)
    print("1.2.0 - 1.5.0 Data Cleaning Pipeline")
    print("-" * 60)
    print("Cleaning steps:")
    print("  1.2.0 Identify and handle missing data (remove, impute, or flag missing values)")
    print("  1.3.0 Detect and handle outliers (remove negatives, cap extremes at 99th percentile)")
    print("  1.4.0 Resolve data inconsistencies:")
    print("    - Standardize column names (lowercase with underscores)")
    print("    - Handle empty strings (replace with NaN)")
    print("    - Standardize categorical values (fix inconsistencies)")
    print("    - Filter to subway records only (YU, BD, SHP)")
    print("  1.5.0 Transform and normalize variables:")
    print("    - Convert data types (dates --> datetime, minutes --> numeric, categories --> category)")
    print("    - Create datetime column and extract time features")
    print("    - Remove records with zero delay or zero gap (not relevant for delay analysis)")
    df_2024_cleaned: Optional[pd.DataFrame] = None
    df_2025_cleaned: Optional[pd.DataFrame] = None

    if df_2024 is not None:
        print("\nCleaning 2024 Dataset")
        df_2024_cleaned = clean_ttc_data(df_2024, remove_outliers=False, outlier_method='cap')

    if df_2025 is not None:
        print("\nCleaning 2025 Dataset")
        df_2025_cleaned = clean_ttc_data(df_2025, remove_outliers=False, outlier_method='cap')

    # Create original (before cleaning) combined dataset for comparison
    df_original: Optional[pd.DataFrame] = None
    if df_2024 is not None and df_2025 is not None:
        # Standardize column names for combination
        df_2024_orig = df_2024.copy()
        df_2025_orig = df_2025.copy()
        df_2024_orig.columns = df_2024_orig.columns.str.lower().str.replace(' ', '_')
        df_2025_orig.columns = df_2025_orig.columns.str.lower().str.replace(' ', '_')

        # Get common columns
        common_cols = list(set(df_2024_orig.columns) & set(df_2025_orig.columns))
        df_2024_orig = df_2024_orig[common_cols]
        df_2025_orig = df_2025_orig[common_cols]

        # Combine original datasets
        df_original = pd.concat([df_2024_orig, df_2025_orig], ignore_index=True)
    elif df_2024 is not None:
        df_original = df_2024.copy()
        df_original.columns = df_original.columns.str.lower().str.replace(' ', '_')
    elif df_2025 is not None:
        df_original = df_2025.copy()
        df_original.columns = df_original.columns.str.lower().str.replace(' ', '_')

    # Combine both datasets (part of cleaning section)
    print("\n")
    print("\nCombining Datasets")
    print("-" * 60)
    df_final: Optional[pd.DataFrame] = None

    if df_2024_cleaned is not None and df_2025_cleaned is not None:
        # Check for _id column and create sequential _id for 2024 data if missing
        if '_id' in df_2025_cleaned.columns and '_id' not in df_2024_cleaned.columns:
            max_id_2025 = df_2025_cleaned['_id'].max() if '_id' in df_2025_cleaned.columns else 0
            df_2024_cleaned['_id'] = range(max_id_2025 + 1, max_id_2025 + 1 + len(df_2024_cleaned))

        # Ensure both dataframes have the same columns in the same order
        common_columns = list(set(df_2024_cleaned.columns) & set(df_2025_cleaned.columns))
        df_2024_cleaned = df_2024_cleaned[common_columns]
        df_2025_cleaned = df_2025_cleaned[common_columns]

        # Combine datasets
        df_combined = pd.concat([df_2024_cleaned, df_2025_cleaned], ignore_index=True)
        print("Combined dataset created.")

        # Store cleaned data for analysis
        df_final = df_combined.copy()

    else:
        # Use whichever dataset is available
        if df_2024_cleaned is not None:
            df_final = df_2024_cleaned.copy()
            print("Using 2024 dataset only.")
        elif df_2025_cleaned is not None:
            df_final = df_2025_cleaned.copy()
            print("Using 2025 dataset only.")
        else:
            df_final = None
            print("No cleaned datasets available.")

    # Combined dataset summary
    if df_final is not None:
        print("\n" + "-" * 60)
        print("Combined Dataset Summary")
        print("-" * 60)
        print(f"Total Records: {len(df_final):,}")
        print(f"Total Columns: {len(df_final.columns)}")

        print("\nMissing Values:")
        missing_summary = df_final.isnull().sum()
        missing_summary = missing_summary[missing_summary > 0]
        if len(missing_summary) > 0:
            for col, count in missing_summary.items():
                print(f"  {col}: {count:,} ({(count/len(df_final)*100):.2f}%)")
        else:
            print("  No missing values!")

        print("\nData Types:")
        for dtype, count in df_final.dtypes.value_counts().items():
            print(f"  {dtype}: {count}")

        print("\nDate Range:")
        if 'datetime' in df_final.columns:
            print(f"  Start: {df_final['datetime'].min()}")
            print(f"  End: {df_final['datetime'].max()}")

        print("\n" + "=" * 70)
        print("1.0.0 Data Preparation Complete - Data is cleaned and ready for analysis!")
        print("=" * 70)

        # Visualize data quality before/after cleaning comparison
        if df_original is not None:
            print("\n" + "-" * 60)
            print("Data Quality: Before/After Cleaning Comparison")
            print("Figure 1: Visual comparison of data quality improvements after cleaning")
            print("-" * 60)
            visualize_data_quality_before_after(df_original, df_final)

    # ============================================================================
    # 2.0.0 DESCRIPTIVE ANALYTICS
    # ============================================================================
    if df_final is not None:
        display_code_categories()
        results_desc = run_descriptive_analytics(df_final)

    # ============================================================================
    # 3.0.0 DIAGNOSTIC ANALYTICS
    # ============================================================================
    if df_final is not None:
        # Run diagnostic analytics on the cleaned dataframe
        results_diag = run_diagnostic_analytics(df_final)

        return df_final, df_original, results_desc, results_diag
    else:
        return None, None, None, None

# ============================================================================
# EXECUTION BLOCK
# ============================================================================

# Run the main analysis pipeline
print("\n" + "=" * 60)
print("Welcome to the TTC Subway Delay Data Analysis Project!")
print("=" * 60)
print("This project provides a comprehensive data analysis pipeline for TTC (Toronto Transit Commission) Subway Delay Data from 2024 and 2025. The analysis follows a structured approach covering data preparation, descriptive analytics, diagnostic analytics, and predictive analytics to identify patterns, relationships, and predict delay severity.")
print("\nKey Objectives:")
print("1. Data Preparation: Clean and prepare raw TTC delay data for analysis")
print("2. Descriptive Analytics: Understand delay patterns, distributions, and trends")
print("3. Diagnostic Analytics: Identify relationships and causes of delays")
print("4. Predictive Analytics: Build models to predict delay severity (short vs. long delays)")
print("5. Visualization: Create comprehensive visualizations for executive presentation.\n")

df_final, df_original, results_desc, results_diag = main()

# run predictive analytics and model visualizations
results_predictive = None
if df_final is not None:
    results_predictive = run_predictive_analytics(df_final)